{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia text 정제 및 문장분리\n",
    "##### 20231110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import stanza\n",
    "# Stanza 초기화\n",
    "stanza.download('id')  # 영어 모델 다운로드\n",
    "nlp = stanza.Pipeline('id', processors='tokenize')\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\USER\\\\Desktop\\\\wiki수동합치기2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset = 'title')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 정규표현식 함수 정의 및 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test(text):\n",
    "\n",
    "    text = re.sub(r\"&nbsp;\", \" \", text)\n",
    "    text = re.sub(r\"&lt;\", \"<\", text)\n",
    "    text = re.sub(r\"&gt;\", \">\", text)\n",
    "    text = re.sub(r\"<ref[^<]*</ref>\", \"\", text)\n",
    "    text = re.sub(r\"\\[\\[[^\\|\\]]*\\|\", \"[[\", text)\n",
    "    text = re.sub(r\"{{[^}]*}}\", \"\", text)\n",
    "    text = re.sub(r\"{[^}]*}\", \"\", text)\n",
    "    text = re.sub(r\"<[^>]*>\", \"\", text)\n",
    "    #text = re.sub(r\"\\|\\d+px\", \"\", text) #remove image px\n",
    "    text = re.sub(r\"==\\s*([^=]+)\\s*=+\", \"\", text) #소제목 제거\n",
    "    text = re.sub(r\"\\=\", \"\", text)\n",
    "    text = re.sub(r\"\\[http[^\\]]+\\]\", \"\", text) #http 제거\n",
    "    text = re.sub(r\"\\[\\[Kategori:[^\\]]+\\]\\]\",\"\", text) #카테고리 제거\n",
    "    text = re.sub(r\"\\{\", \"\", text)\n",
    "    text = re.sub(r\"\\}\", \"\", text)\n",
    "    text = re.sub(r\"\\\\'\",\"\", text) #wiki에서 글씨체 효과 제거\n",
    "    text = re.sub(r\"\\'''\", \"\", text) #wiki에서 bold처리 표현식 제거\n",
    "    text = re.sub(r\"\\''\", \"\", text) #wiki에서 이탤릭체 표현식 제거\n",
    "    text = re.sub(r\"\\#\", \"\", text) #wiki 숫자 표시 제거\n",
    "    text = re.sub(r\"\\*\", \"\", text) #wiki 점 제거\n",
    "    text = re.sub(r\"\\[\", \"\", text)\n",
    "    text = re.sub(r\"\\]\", \"\", text)\n",
    "    #text = re.sub(r\"\\|[^|]+\\|\", \"|\", text)\n",
    "    #text = re.sub(r\"\\|.*? |\\|\\d+px\", \"\", text) #remove | and image px\n",
    "    text = re.sub(r'jmpl\\|', '', text)\n",
    "    text = re.sub(r'ka\\|', '', text)\n",
    "    text = re.sub(r'\\d+px\\|', '', text)\n",
    "    text = re.sub(r'kiri\\|', '', text)\n",
    "    text = re.sub(r'thumb\\|', '', text)\n",
    "    text = re.sub(r'lurus\\|', '', text)\n",
    "    text = re.sub(r'left\\|', '', text)\n",
    "    text = re.sub(r'right\\|', '', text)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\"\",text)\n",
    "    text = re.sub(r\"\\|.*?   |\\|\\d+px\",\"\",text)\n",
    "    text = re.sub(r\"\\|.*?  |\\|\\d+px\",\"\",text)\n",
    "    text = re.sub(r\"\\|.*? |\\|\\d+px\",\"\",text)\n",
    "    text = re.sub(r\"\\|.*?|\\|\\d+px\",\"\",text)\n",
    "    text = re.sub(r\"\\d+px\", \"\",text) #remove image px\n",
    "    text = re.sub(r\"[가-힣]+\",\"\",text) #remove 한글\n",
    "    text = re.sub(r\"[一-龥]+\",\"\",text) #remove 한자\n",
    "    text = re.sub(r\"\\([^)]*\\)\",\"\",text) #remove 소괄호, 소괄호 속 내용\n",
    "    text = re.sub(r\"http\\S+\", \"\", text) #remove http\n",
    "    text = re.sub(r\"\\!\",\"\",text)\n",
    "    text = re.sub(r'.*?\\.jpg\\s*', '', text) #jpg 앞쪽 문장 모두 제거\n",
    "    text = re.sub(r'jpg', '', text)\n",
    "    text = re.sub(r'Berkas:.*',\"\",text) #Berkas: 로 문장 시작 시 모두 제거\n",
    "    text = re.sub(r'\\s*section at the bottom of the .*$','',text)\n",
    "    text = re.sub(r'text = re.sub(r\"[\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF『』、。《》]\", \"\", text)','',text) #remove 일본어\n",
    "    text = re.sub(r'-->',\"\",text)\n",
    "    text = re.sub(r'-',\"\",text)\n",
    "    text = re.sub(r'::',\"\",text)\n",
    "    text = re.sub(r'《',\"\",text)\n",
    "    text = re.sub(r'》',\"\",text)\n",
    "    text = re.sub(r'『',\"\",text)\n",
    "    text = re.sub(r'』',\"\",text)\n",
    "    text = re.sub(r'&x7C;',\"\",text)\n",
    "    \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 정규표현식 함수 적용 전 NaN 값 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN 값을 처리합니다. 예를 들어, NaN 값을 빈 문자열로 대체합니다.\n",
    "df['text'] = df['text'].fillna('')\n",
    "\n",
    "# 모든 값을 문자열로 변환합니다.\n",
    "df['text'] = df['text'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].fillna('')\n",
    "\n",
    "# 모든 값을 문자열로 변환합니다.\n",
    "df['title'] = df['title'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'text' 열에 정규표현식 함수 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleansing_test'] = df['text'].apply(process_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 정제된 문장 분리 및 토크나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(df):\n",
    "    max_length=5\n",
    "    new_rows = []\n",
    "    index=0\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\"):\n",
    "        index+=1\n",
    "        article_text = row['cleansing_test']\n",
    "        doc = nlp(article_text)\n",
    "        sen_id = 1  # 문장 ID를 1로 초기화\n",
    "        for sentence in doc.sentences:\n",
    "            new_row = row.copy()\n",
    "            new_row['Sentence'] = sentence.text\n",
    "            new_row['Token']= [token.text for token in sentence.tokens]\n",
    "            new_row['Word_Count'] = len(new_row['Token'])\n",
    "            new_row['Doc_ID']='20230816_wikidata_'+new_row['title']+'_000001'\n",
    "            new_row['Sen_ID'] = new_row['Doc_ID']+'_sen'+str('{:06d}'.format(sen_id))\n",
    "            new_row['Pub_Date']=new_row['date'][:10]\n",
    "            new_row['Title']=new_row['title']\n",
    "            new_row['Pub_Subj']=new_row['title']\n",
    "            new_rows.append(new_row)\n",
    "            sen_id += 1  # 문장 ID 증가\n",
    "\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    new_df['Tokenized_Sentence']= new_df['Token'].apply(' '.join)\n",
    "    new_df['Col_Date']='2023-08-10'\n",
    "    new_df['Pub_Type']='Wikipedia'\n",
    "    new_df['Text']=new_df['cleansing_test']\n",
    "    new_df['Filename']='C:/Users/Juhee Kim/Ars Praxia/[T] 23-05-001 NIA AI 데이터셋 구축 - General/06-수집데이터/위키 데이터/rawdata/idwiki-20230720-pages-articles-multistream.xml'\n",
    "\n",
    "    new_df.drop_duplicates(subset='Sentence', inplace=True)\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "    new_df = new_df[['Doc_ID', 'Filename', 'Title', 'Pub_Type', 'Pub_Subj', 'Pub_Date', 'Col_Date', 'Sen_ID', 'Word_Count', 'Text', 'Sentence','Tokenized_Sentence' , 'Token']]\n",
    "    new_df = new_df[new_df['Token'].apply(lambda x: len(x) > max_length)]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test= tokenizing(df)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2235631"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['Word_Count'].sum() #총 토큰 수 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 테스트웍스(가공 담당 회사)전달 전 각 Token에 대한 column 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_test[df_test['Word_Count']<=200]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast  # 문자열을 리스트로 변환하는 데 사용\n",
    "\n",
    "# 빈 데이터프레임 리스트 초기화\n",
    "temp_dfs = []\n",
    "\n",
    "# Token 열의 문자열을 실제 리스트로 변환\n",
    "df['Token'] = df['Token'].apply(ast.literal_eval)\n",
    "\n",
    "# 가장 많은 토큰을 가진 행의 토큰 개수를 찾음\n",
    "max_tokens = df['Token'].apply(len).max()\n",
    "\n",
    "# 필요한 만큼의 새 열을 미리 생성\n",
    "for i in range(max_tokens):\n",
    "    df[f'Column_{i}'] = None\n",
    "\n",
    "# 각 행에 대해 반복\n",
    "for idx in tqdm(df.index):\n",
    "    # 해당 행의 토큰 리스트\n",
    "    tokens = df.loc[idx, 'Token']\n",
    "\n",
    "    # 각 토큰에 대해 반복\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 해당 토큰을 적절한 열에 할당\n",
    "        df.at[idx, f'Column_{i}'] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위 코드 에러 발생 시 이 코드로 컬럼 생성\n",
    "\n",
    "# 토큰별 작업용 컬럼 생성\n",
    "for i in range(df['Word_Count'].max()):\n",
    "    new_column_name = f'Column_{i}'  # 새 열 이름 생성\n",
    "    df[new_column_name]=np.nan\n",
    "\n",
    "for idx in tqdm(df.index):\n",
    "    for i, token in enumerate(df['Token'][idx]):\n",
    "        new_column_name = f'Column_{i}'  # 새 열 이름 생성\n",
    "        df[new_column_name][idx]=token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Sen_ID'기준으로 정렬\n",
    "df.sort_values('Sen_ID', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('20231110_원천데이터(위키).csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('20231110_원천데이터(위키).pickle') #용량 클 시 pickle 파일로 저장"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
