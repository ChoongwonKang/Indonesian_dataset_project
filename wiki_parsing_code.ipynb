{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "from six.moves import urllib\n",
    "from time import time\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmlfile='idwiki-20230720-pages-articles-multistream.xml'\n",
    "ns = {'export-0.1': 'http://www.mediawiki.org/xml/export-0.10/'}\n",
    "tags_to_skip = [\"siteinfo\"]\n",
    "\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_wiki_xml(filename):\n",
    "    # tags_to_skip는 건너뛸 태그의 목록입니다. 예를 들어, \"siteinfo\" 태그 등을 건너뛰는 용도로 사용됩니다.\n",
    "    skipping = \"\"  # 현재 건너뛰고 있는 태그를 저장하는 변수\n",
    "    in_page = False  # 현재 'page' 태그 내부에 있는지 여부를 나타내는 변수\n",
    "\n",
    "    # XML 파일을 파싱하면서 start와 end 이벤트를 반복적으로 처리합니다.\n",
    "    for event, elem in ET.iterparse(filename, events=(\"start\", \"end\",)):\n",
    "        if event == \"start\":\n",
    "            # tags_to_skip에 해당하는 태그가 있다면, 해당 태그를 건너뜁니다.\n",
    "            for tag in tags_to_skip:\n",
    "                if tag in elem.tag:\n",
    "                    print(\"removing elem siteinfo\")  # 현재 siteinfo 태그를 건너뛰고 있다는 메시지를 출력합니다.\n",
    "                    skipping = tag\n",
    "                    elem.clear()  # 건너뛰는 태그를 제거합니다.\n",
    "                    break\n",
    "            if in_page:\n",
    "                continue\n",
    "            if \"page\" in elem.tag:\n",
    "                in_page = True  # 'page' 태그 내부에 들어왔으므로 in_page를 True로 설정합니다.\n",
    "        elif event == \"end\":\n",
    "            if skipping:\n",
    "                # 현재 건너뛰고 있는 태그를 처리합니다.\n",
    "                if skipping in elem.tag:\n",
    "                    elem.clear()  # 건너뛰는 태그를 제거합니다.\n",
    "                    skipping = \"\"  # skipping 변수를 초기화합니다.\n",
    "            else:\n",
    "                # 건너뛰는 태그가 아니라면 'page' 태그를 처리합니다.\n",
    "                if \"page\" in elem.tag:\n",
    "                    yield elem  # 'page' 태그를 반환합니다.\n",
    "                    elem.clear()  # 'page' 태그를 처리했으므로 해당 태그를 제거합니다.\n",
    "                    in_page = False  # 'page' 태그 내부를 빠져나왔으므로 in_page를 False로 설정합니다.\n",
    "                    \n",
    "\n",
    "pages = parse_wiki_xml(xmlfile)\n",
    "\n",
    "def process_text(text):\n",
    "    # Remove any text not normally visible\n",
    "    text = re.sub(r\"&amp;\", \"&\", text)  # decode URL encoded chars\n",
    "    text = re.sub(r\"&nbsp;\", \" \", text) #html에서의 공백 제거\n",
    "    text = re.sub(r\"&lt;\", \"<\", text)\n",
    "    text = re.sub(r\"&gt;\", \">\", text)\n",
    "    text = re.sub(r\"<.*>\", \"\", text)  # remove xml tags\n",
    "    text = re.sub(r\"<ref[^<]*</ref>\", \"\", text)  # remove references <ref...> ... </ref>\n",
    "    text = re.sub(r\"<[^>]*>\", \"\", text)  # remove xhtml tags\n",
    "    text = re.sub(r\"\\[http:[^] ]*\", \"[\", text)  # remove normal url, preserve visible text\n",
    "    text = re.sub(r\"\\|thumb\", \"\", text)  # remove images links, preserve caption\n",
    "    text = re.sub(r\"\\|left\", \"\", text)\n",
    "    text = re.sub(r\"\\|right\", \"\", text)\n",
    "    text = re.sub(r\"\\|\\d+px\", \"\", text)\n",
    "    text = re.sub(r\"\\[\\[image:[^\\[\\]]*\\|\", \"\", text)\n",
    "    text = re.sub(r\"\\[\\[category:([^|\\]]*)[^]]*\\]\\]\", r\"\\1\", text, flags=re.I)  # show categories without markup\n",
    "    text = re.sub(r\"\\[\\[[a-z\\-]*:[^\\]]*\\]\\]\", \"\", text)  # remove links to other languages\n",
    "    text = re.sub(r\"\\[\\[[^\\|\\]]*\\|\", \"[[\", text)  # remove wiki url, preserve visible text\n",
    "    text = re.sub(r\"{{[^}]*}}\", \"\", text)  # remove {{icons}} and {tables}\n",
    "    text = re.sub(r\"{[^}]*}\", \"\", text)\n",
    "    text = re.sub(r\"\\}\", \"\", text)\n",
    "    text = re.sub(r\"\\[\", \"\", text)  # remove [ and ]\n",
    "    text = re.sub(r\"\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\(\", \"\", text)  # remove ( and )\n",
    "    text = re.sub(r\"\\)\", \"\", text)\n",
    "    text = re.sub(r\"&[^;]*;\", \" \", text)  # remove URL encoded chars\n",
    "    text = re.sub(r\"\\\"\", \"\", text)  # remove ' and \"\n",
    "    text = re.sub(r\"'\", \"\", text)\n",
    "    text = re.sub(r\"_\", \"\", text)  # remove _\n",
    "    text = re.sub(r\"\\W+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def convert_to_df(pages):\n",
    "    data = []\n",
    "    t0 = time()\n",
    "    for i, page in enumerate(pages):\n",
    "        if i % 2000 == 1999:\n",
    "            print(\"Read {}k articles. Elapsed time: {:.3f}s\".format(int((i+1)/1000), time() - t0), end=\"\\r\")\n",
    "\n",
    "        title = page.find('export-0.1:title', ns).text.lower()\n",
    "        \n",
    "        text = page.find('export-0.1:revision', ns).find('export-0.1:text', ns).text\n",
    "        if not text:\n",
    "            page.clear()\n",
    "            continue\n",
    "\n",
    "        text = process_text(text)\n",
    "\n",
    "        words = text.split()\n",
    "        \n",
    "        total_words = len(words)\n",
    "        total_long_words = len([w for w in words if len(w) > 3])\n",
    "        if total_long_words < 15:\n",
    "            page.clear()\n",
    "            continue\n",
    "        date = page.find('export-0.1:revision', ns).find('export-0.1:timestamp', ns).text\n",
    "        text =  \" \".join(words)\n",
    "        data.append([title, date, text])\n",
    "\n",
    "    # Membuat data frame dari list data\n",
    "    columns = ['title', 'date', 'text']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML 파일 파싱\n",
    "pages = parse_wiki_xml(xmlfile)\n",
    "\n",
    "# 파싱된 페이지를 데이터 프레임으로 변환\n",
    "df = convert_to_df(pages)\n",
    "\n",
    "# 위키미디어 데이터를 JSON 형식으로 변환하여 텍스트 파일로 저장\n",
    "text_filename = os.path.splitext(xmlfile)[0] + \".txt\"\n",
    "print(\"Start writing Wikipedia texts to {}\".format(text_filename))\n",
    "t0 = time()\n",
    "\n",
    "with open(text_filename, \"wb\") as f:\n",
    "    for i, page in enumerate(pages):\n",
    "        text = process_text(page.find('export-0.1:revision', ns).find('export-0.1:text', ns).text)\n",
    "        f.write((json.dumps({\"text\": text}) + os.linesep).encode(\"utf-8\"))\n",
    "        if i % 1000 == 999:\n",
    "            print(\"Done writing {}k pages. Elapsed time: {:.3f}s\".format(int((i+1)/1000), time() - t0), end=\"\\r\")\n",
    "print(\"Done writing Wikipedia texts in {:.3f}s\".format(time() - t0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
