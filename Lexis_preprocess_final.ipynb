{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LexisNexis 기사 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4651b292714e26a318318638d08bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 10:19:08 INFO: Downloading default packages for language: id (Indonesian) ...\n",
      "2023-11-22 10:19:09 INFO: File exists: C:\\Users\\USER\\stanza_resources\\id\\default.zip\n",
      "2023-11-22 10:19:13 INFO: Finished downloading models and saved to C:\\Users\\USER\\stanza_resources.\n",
      "2023-11-22 10:19:13 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892cd4dc6ec041b68d04e1dc6f405e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 10:19:14 WARNING: Language id package default expects mwt, which has been added\n",
      "2023-11-22 10:19:14 INFO: Loading these models for language: id (Indonesian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "=======================\n",
      "\n",
      "2023-11-22 10:19:14 INFO: Using device: cpu\n",
      "2023-11-22 10:19:14 INFO: Loading: tokenize\n",
      "2023-11-22 10:19:14 INFO: Loading: mwt\n",
      "2023-11-22 10:19:14 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import stanza\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "# Stanza 초기화\n",
    "stanza.download('id')  # 영어 모델 다운로드\n",
    "\n",
    "# 스탄자 인도네시아어 모델 로드\n",
    "stanza_id = stanza.Pipeline('id', processors='tokenize')\n",
    "\n",
    "# 스파이시 영어 모델 로드\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 함수 정의\n",
    "##### - 파일 읽기\n",
    "##### - 데이터 전처리 및 변환\n",
    "##### - 메타데이터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    global date, keyword\n",
    "    \n",
    "    # 파일명에서 날짜와 도시 이름을 추출하는 정규표현식 패턴\n",
    "    pattern = re.compile(r'(\\d{8})_newsdata_(.*)\\.csv')\n",
    "    match = re.match(pattern, file_name)\n",
    "\n",
    "    if match:\n",
    "        date = match.group(1)  # 추출한 날짜\n",
    "        keyword = match.group(2)  # 추출한 도시 이름\n",
    "        print(\"Date:\", date)\n",
    "        print(\"Keyword:\", keyword)\n",
    "    else:\n",
    "        print(\"No match found\")\n",
    "\n",
    "\n",
    "    df= pd.read_csv(file_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "#수집일 및 키워드 표기\n",
    "def set_metadata(df):\n",
    "\n",
    "    date_str = date\n",
    "\n",
    "    # YYYY-MM-DD 형태로 수집 날짜 명시\n",
    "    formatted_date = f\"{date[:4]}-{date[4:6]}-{date[6:]}\"\n",
    "    print(\"collection date :\", formatted_date)\n",
    "\n",
    "    # 파일명에서 확장자(.csv)를 제외한 부분 선택\n",
    "    prefix = file_name.split('.')[0]\n",
    "    \n",
    "    df['Doc_ID'] = prefix + \"_\" + df['ID'].apply(format_id).astype(str)\n",
    "    df['Col_Date']= formatted_date\n",
    "    df['Keyword']=keyword\n",
    "    \n",
    "    for idx in df.index :\n",
    "        if type(df['Date'][idx]) == float:\n",
    "            # Check if it's the first row\n",
    "            if idx == df.index[0]:\n",
    "                # Handle the first row differently. For example, set it to NaN or a default date.\n",
    "                df['Date'][idx] = pd.NaT\n",
    "            else:\n",
    "                df['Date'][idx] = df['Date'][idx-1]\n",
    "\n",
    "\n",
    "\n",
    "def cleansing(text):\n",
    "    # \"MMM DD, YYYY\" 형식의 날짜와 \")\"로 끝나는 부분을 찾아 제거하는 정규표현식\n",
    "    pattern_1 = r'\\w{3}\\s\\d{2},\\s\\d{4}\\(.*?\\)'\n",
    "    pattern_2 = r'Loading Link to the original story\\. Notes.*?sole discretion\\.'\n",
    "    pattern_3 = r'\\b[A-Z][a-z]{2} \\d{2}, \\d{4} \\(.*?by Newstex\\)'\n",
    "    \n",
    "    # 정규표현식을 사용하여 패턴을 제거\n",
    "    cleaned_text = re.sub(pattern_3, '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # cleaned_text = re.sub(pattern_1, '', text)\n",
    "\n",
    "    # 왼쪽 공백 제거\n",
    "    cleaned_text = cleaned_text.lstrip()\n",
    "\n",
    "    if text.endswith('sole discretion.'):\n",
    "        cleaned_text= cleaned_text[:-1284]\n",
    "    \n",
    "\n",
    "    # 날짜 형식 제거 (예: Apr 08, 2013)\n",
    "    cleaned_text = re.sub(r'\\b[A-Z][a-z]{2} \\d{2}, \\d{4}\\b', '', cleaned_text)\n",
    "    # http:// 또는 https:// 형태 제거\n",
    "    cleaned_text= re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', cleaned_text)\n",
    "    # 대괄호 안 숫자 제거\n",
    "    cleaned_text = re.sub(r'\\[\\d+\\]', '', cleaned_text)\n",
    "    # <--> 형태 제거\n",
    "    cleaned_text = re.sub(r'<.*?>', '', cleaned_text)\n",
    "    \n",
    "\n",
    "    # 정규표현식 패턴을 사용하여 해당 패턴 제거\n",
    "    pic_pattern = re.compile(r'pic\\.twitter\\.com\\/[A-Za-z0-9]+')\n",
    "    cleaned_text = re.sub(pic_pattern, \"\", cleaned_text)\n",
    "\n",
    "\n",
    "    # 한글 제거\n",
    "    text_without_korean = re.sub('[가-힣]+', '', cleaned_text)\n",
    "    ko_pattern = re.compile(r'[ㄱ-ㅎㅏ-ㅣ]+')\n",
    "    text_without_korean = re.sub(ko_pattern, '', text_without_korean)\n",
    "    \n",
    "    \n",
    "    # 링크 제거 (http:// 또는 https://로 시작하는 링크)\n",
    "    text_without_links = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text_without_korean)\n",
    "    if (text_without_links.startswith(\"(\")) & (\")\" in text_without_links):\n",
    "        end_index = text_without_links.index(')')\n",
    "        text_without_links = text_without_links[end_index+1:]\n",
    "    if text_without_links.startswith(\"(KoreanIndo: Delivered by Newstex)\"):\n",
    "        text_without_links = text_without_links[len(\"(KoreanIndo: Delivered by Newstex)\"):]\n",
    "    \n",
    "    \n",
    "    pattern = re.compile(r'(source|Source)')\n",
    "    match = pattern.search(text_without_links)\n",
    "\n",
    "    \n",
    "    if match:\n",
    "        text_without_links = re.split(r'(source|Source).*', text_without_links)[0].strip()\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    text_without_links = text_without_links.replace(\"Loading\", \"\")\n",
    "    \n",
    "    \n",
    "    text_without_links = text_without_links.replace(\"[embedded content]\", \"\")\n",
    "    text_without_links = re.sub(r'\\[ \\d+\\]:', '', text_without_links)\n",
    "    return text_without_links\n",
    "\n",
    "\n",
    "def cleansing_text(test):\n",
    "\n",
    "    test = re.sub(r\"Baca Juga:.*?\\.\", \"\", test) #Baca Juga 관련 문장 제거\n",
    "    test = re.sub(r\"Baca Juga :.*?\\.\", \"\", test)\n",
    "    test = re.sub(r\"Baca juga:.*?\\.\", \"\", test)\n",
    "    test = re.sub(r\"Baca juga.*?\\.\", \"\", test)\n",
    "    test = re.sub(r\"Baca juga :.*?\\.\", \"\", test)\n",
    "    test = re.sub(r\"baca juga:.*?\\.\", \"\", test)\n",
    "    test = re.sub(r\"baca juga :.*?\\.\", \"\", test)\n",
    "    test = re.sub(r\"baca Juga:.*?\\.\", \"\", test)\n",
    "    test = re.sub(r\"baca Juga :.*?\\.\", \"\", test)\n",
    "    test = re.sub(r\"Foto:.*?\\.\",\"\",test) #Foto 관련 문장 제거\n",
    "    test = re.sub(r\"Written by:.*\",\"\",test) #Written by: 이후 모든 문장 제거\n",
    "    test = re.sub(r\"Written by :.*\",\"\",test)\n",
    "    test = re.sub(r\"written by:.*\",\"\",test)\n",
    "    test = re.sub(r\"written by :.*\",\"\",test)\n",
    "    test = re.sub(r\"written by .*\",\"\",test)\n",
    "    test = re.sub(r\"—- Written by :.*\",\"\",test)\n",
    "    test = re.sub(r\"—Written by :.*\",\"\",test)\n",
    "    test = re.sub(r\"\\b[A-Z][a-z]{2} \\d{2}, \\d{4} \\(.*?by Newstex\\)\",\"\",test) #날짜 제거\n",
    "    test = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\"\",test) #링크 제거\n",
    "    test = re.sub(r\"\\[\\d+\\]\",\"\",test) #대괄호 포함 내용 제거\n",
    "    test = re.sub(r\"\\[ \\d+\\]\",\"\",test) #대괄호 포함 내용 제거\n",
    "    test = re.sub(r\":\",\"\",test)\n",
    "    test = re.sub(r\"<.*?>\",\"\",test)\n",
    "    test = re.sub(r\"//instagram.com/.*\",\"\",test) #//instagram.com/ 이후 모든 문장 제거\n",
    "    test = re.sub(r\"\\b(?:\\w+\\W+){0,30}Soompi(?:\\W+\\w+){0,30}\\b\", \"\", test) #Soompi 기준 앞 뒤 단어 30개 제거\n",
    "    test = re.sub(r\"\\b(?:\\w+\\W+){0,30}soompi(?:\\W+\\w+){0,30}\\b\",\"\",test) #soompi\n",
    "    test = re.sub(r'#\\d+;', \"\", test) ##8216;, #8217; 등 제거\n",
    "    test = re.sub(r'# \\d+;', \"\", test)\n",
    "    test = re.sub(r'#\\d+ ;', \"\", test)\n",
    "    test = re.sub(r'# \\d+ ;', \"\", test)\n",
    "        \n",
    "    return test\n",
    "\n",
    "\n",
    "def second_cleansing_text(dtest):\n",
    "\n",
    "    dtest = re.sub(r\"\\[ \\d+\\]\",\"\",dtest)\n",
    "    dtest = re.sub(r\"Written by:.*\",\"\",dtest) #Written by: 이후 모든 문장 제거\n",
    "    dtest = re.sub(r\"Written by :.*\",\"\",dtest)\n",
    "    dtest = re.sub(r\"written by:.*\",\"\",dtest)\n",
    "    dtest = re.sub(r\"written by :.*\",\"\",dtest)\n",
    "    dtest = re.sub(r\"//instagram.com/.*\",\"\",dtest) #//instagram.com/ 이후 모든 문장 제거\n",
    "    dtest = re.sub(r\"\\b(?:\\w+\\W+){0,30}Soompi(?:\\W+\\w+){0,30}\\b\", \"\", dtest) #Soompi 기준 앞 뒤 단어 30개 제거\n",
    "    dtest = re.sub(r\"\\b(?:\\w+\\W+){0,30}soompi(?:\\W+\\w+){0,30}\\b\",\"\",dtest) #soompi\n",
    "    dtest = re.sub(r\":\",\"\",dtest)\n",
    "    dtest = re.sub(r\"[一-龥]+\",\"\",dtest)\n",
    "\n",
    "    return dtest\n",
    "\n",
    "\n",
    "# 1 -> 000001, 10231 -> 010231 형태로 id number 바꾸기\n",
    "def format_id(id_number):\n",
    "    # Check if the value is NaN\n",
    "    if pd.isna(id_number):\n",
    "        return 'NaN'\n",
    "    \n",
    "    formatted_id = '{:06d}'.format(int(id_number))\n",
    "    return formatted_id\n",
    "\n",
    "\n",
    "# 'Article' Column에 있는 기사 본문의 metadata 명시 및 불필요 텍스트 제거 후 'Text' Column으로 생성\n",
    "def preprocessing(df) :\n",
    "    set_metadata(df)\n",
    "    df['Text'] = df['Article'].apply(cleansing_text)\n",
    "    df['Text'] = df['Text'].apply(cleansing)\n",
    "    df['Text'] = df['Text'].apply(second_cleansing_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터프레임 형태로 폴더 내부 원시데이터 파일 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\USER\\\\Desktop\\\\test'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mChoongwonKang\\Indonesian_dataset_project\\Lexis_preprocess_final.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell://github/ChoongwonKang/Indonesian_dataset_project/Lexis_preprocess_final.ipynb#X10sdnNjb2RlLXZmcw%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 현재 폴더 내의 파일 리스트 얻기\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell://github/ChoongwonKang/Indonesian_dataset_project/Lexis_preprocess_final.ipynb#X10sdnNjb2RlLXZmcw%3D%3D?line=1'>2</a>\u001b[0m file_list \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir()\n\u001b[0;32m      <a href='vscode-notebook-cell://github/ChoongwonKang/Indonesian_dataset_project/Lexis_preprocess_final.ipynb#X10sdnNjb2RlLXZmcw%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# 폴더 내 파일들에 대해 반복하여 CSV 파일 읽기\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell://github/ChoongwonKang/Indonesian_dataset_project/Lexis_preprocess_final.ipynb#X10sdnNjb2RlLXZmcw%3D%3D?line=4'>5</a>\u001b[0m df_all\u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# 현재 폴더 내의 파일 리스트 얻기\n",
    "file_list = os.listdir()\n",
    "\n",
    "# 폴더 내 파일들에 대해 반복하여 CSV 파일 읽기\n",
    "df_all= pd.DataFrame()\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "for file_name in tqdm(file_list):\n",
    "    if file_name.endswith('.csv'):\n",
    "        new_df = read_file(file_name)\n",
    "        print(f'filename : {file_name}')\n",
    "        print(f\"length : {len(new_df)}\")\n",
    "        preprocessing(new_df)\n",
    "\n",
    "    df_all =pd.concat([df_all, new_df], axis=0)\n",
    "print(\"-----------------------------------\\n\")\n",
    "\n",
    "# news_list에 있는 키워드가 들어간 요소들 추리기\n",
    "news_list= ['KoreanIndo', 'Newstex Blogs', 'The Conversation (Indonesia Edition)', 'Mobile88.com', 'GlobeNewswire Indonesian',\n",
    "             'ParsToday (Indonesian)', 'Plus Company Updates(PCU)', 'Mongabay.com - Indonesian', 'Thomson Reuters ONE',\n",
    "               'Koreana', 'Webnews - Indonesian', 'Berita Harian', 'ParsToday (Indonesian)', 'Berita Dalam Negeri',\n",
    "                 'TendersInfo - Contract Awards', 'Antara News', 'Industry SnapShot',\n",
    "                   'Plus Company Updates(PCU)', 'Berita Ekonomi', 'ASAPII Database', 'Channel NewsAsia', 'BruDirect',\n",
    "                     'US Official News' , 'Nepal Weekly', 'The New York Times', 'Premium Official News',\n",
    "                      'Media OutReach Newswire (Indonesian)', 'US Fed News', 'States News Service', 'CNN Indonesia', 'M2 PressWIRE', \n",
    "                        'Asia News Monitor', 'Global English (Middle East and North Africa Financial Network)', 'Indonesia Tribune', \n",
    "                          'News Direct', 'MENAFN -Press Releases (English)', 'Contify Insurance News', 'Islamic Finance Monitor Worldwide', 'Indonesia Government News', 'ICT Monitor Worldwide'\n",
    "                            ]\n",
    "df_all= df_all[df_all['Newspaper'].isin(news_list)]\n",
    "\n",
    "# 중복 제거 후 날짜 기준으로 정렬\n",
    "df_all.drop_duplicates(subset='Text', inplace=True)\n",
    "df_all.sort_values('Date', inplace=True)\n",
    "df_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 토크나이징 함수 정의\n",
    "##### - 'Text' 문장 분리 진행 후 'Sentence' 열에 분리된 문장 나열\n",
    "##### - 'Sentence' 열의 인도네시아 단어들을 전처리 및 토큰화 진행 후 'Token'열에 나열 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(df):\n",
    "    new_rows = []\n",
    "    max_length = 5\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\"):\n",
    "        article_text = row['Text']\n",
    "        doc = nlp(article_text)\n",
    "        sen_id = 1  # 문장 ID를 1로 초기화\n",
    "\n",
    "        for sentence in doc.sents:  # spacy에서는 .sents로 문장에 접근\n",
    "            sentence = re.sub(r'Here.*website:', \"\", sentence.text)\n",
    "            sentence = re.sub(r'shared from .*', \"\", sentence.text)\n",
    "            sentence = re.sub(r'Shared from .*', \"\", sentence)\n",
    "            sentence = re.sub(r'shared by .*', \"\", sentence)\n",
    "            sentence = re.sub(r'Shared by .*', \"\", sentence)\n",
    "            sentence = re.sub(r'&#\\d+;', \"\", sentence) #remove &#38;(문장 부호 유형)\n",
    "            sentence = re.sub(r'#\\d+;', \"\", sentence) #remove #8216;, #8217; etc(문장 부호 유형)\n",
    "            sentence = re.sub(r'# \\d+;', \"\", sentence)\n",
    "            sentence = re.sub(r'#\\d+ ;', \"\", sentence)\n",
    "            sentence = re.sub(r'# \\d+ ;', \"\", sentence)\n",
    "            sentence = re.sub(r\"'#\\d+;[^']+'\", \"\", sentence)\n",
    "            sentence = re.sub(r\"ig_mid=[A-Z0-9\\-]+\", \"\", sentence) # #38; 뒤에 오는 ig_mid= 형태의 문장 제거(문장 부호 유형)\n",
    "            sentence = re.sub(r'\\[ \\d+ \\] : .*',\"\",sentence) #remove [ 1 ] : [ 2 ] : [ 3 ] etc\n",
    "            sentence = re.sub(r\"Written by:.*\",\"\",sentence) #Written by: 이후 모든 문장 제거(출처&링크 유형)\n",
    "            sentence = re.sub(r\"Written by :.*\",\"\",sentence)\n",
    "            sentence = re.sub(r\"Written by.*\",\"\",sentence)\n",
    "            sentence = re.sub(r\"written by:.*\",\"\",sentence)\n",
    "            sentence = re.sub(r\"written by :.*\",\"\",sentence)\n",
    "            sentence = re.sub(r\"written by .*\",\"\",sentence)\n",
    "            sentence = re.sub(r\"—- Written by :.*\",\"\",sentence)\n",
    "            sentence = re.sub(r\"—Written by :.*\",\"\",sentence)\n",
    "            sentence = re.sub(r\"#main #main #main.*\",\"\",sentence) # #main 이후 모든 문장 제거\n",
    "            sentence = re.sub(r\"About these ads.*\",\"\",sentence) #About these ads. 이후 모든 문장 제거(광고 유형)\n",
    "            sentence = re.sub(r\"Allkpop.*\",\"\",sentence) #Allkpop 이후 모든 문장 제거(제거 시 가끔 이메일 주소도 같이 삭제)\n",
    "            sentence = re.sub(r\"allkpop.*\",\"\",sentence)\n",
    "            sentence = re.sub(r\"Indotrans.*\",\"\",sentence) #allkpop 바로 뒤에 나오는 indotrans 이후 모든 문장 제거\n",
    "            sentence = re.sub(r\"indotrans.*\",\"\",sentence)\n",
    "            sentence = re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.(com|net|kr)\\b\",\"\",sentence) #remove email(.com, .net, .kr)\n",
    "            sentence = re.sub(r\"\\([^@]+@[^\\)]+\\)\",\"\",sentence) #(@e-mail) 형태 문장 제거\n",
    "            sentence = re.sub(r\"\\(@[^)]+\\)\",\"\",sentence) #(@단어) 형태 문장 제거(SNS 계정 포함)\n",
    "            sentence = re.sub(r\"\\( @[^)]+\\)\",\"\",sentence) \n",
    "            sentence = re.sub(r\"\\(#[^)]+\\)\",\"\",sentence) #(#단어) 형태 문장 제거\n",
    "            sentence = re.sub(r\"@\\w+\\b\",\"\",sentence) #@포함 바로 뒤에 오는 단어 제거\n",
    "            sentence = re.sub(r\"#\\w+\\b\",\"\",sentence) # #포함 바로 뒤에 오는 단어 제거\n",
    "            sentence = re.sub(r\"utm_medium.*\",\"\",sentence) #utm_medium 이후 모든 문장 제거 (Soompi 관련 text)\n",
    "            sentence = re.sub(r\"utm_campaign.*\",\"\",sentence)\n",
    "            sentence = re.sub(r\"Photo Credit.*\",\"\",sentence) #Photo Credit/credit 이후 모든 문장 재거\n",
    "            sentence = re.sub(r\"Photo credit.*\",\"\",sentence)\n",
    "            sentence = re.sub(r\"Article credit.*\",\"\",sentence)\n",
    "            sentence = re.sub(r\"Lihat videonya.*\",\"\",sentence) #광고문 뒷문장 모두 제거\n",
    "            sentence = re.sub(r\"lihat videonya.*\",\"\",sentence) \n",
    "            sentence = re.sub(r\"^.*\\d+\\s*points\\s*\\d+.*$\",\"\",sentence) #'숫자 points 숫자' 형태의 문장 제거(고유명사 나열 유형)\n",
    "            sentence = re.sub(r\"^.*\\d+\\s*Points\\s*\\d+.*$\",\"\",sentence) \n",
    "            sentence = re.sub(r\"^.*\\d+\\s*points\\s*\\.*$\",\"\",sentence)\n",
    "            sentence = re.sub(r\"^.*\\d+\\s*poin.*$\",\"\",sentence) # '숫자 poin' 형태의 문장 제거(고유명사 나열 유형)\n",
    "            sentence = re.sub(r\"^.*\\d+\\s*Poin.*$\",\"\",sentence)\n",
    "            sentence = re.sub(r\"^.*berikut\\sini\\.\\s*$\",\"\",sentence) #텍스트 마지막에 존재하는 berikut ini. 가 포함된 문장 제거(인니어 광고문 유형)\n",
    "            sentence = re.sub(r\"^.*bawah ini.*$\",\"\",sentence) #di bawah ini가 포함된 문장 제거(인니어 광고문 유형)\n",
    "            sentence = re.sub(r\"^.*\\.\\.\\.$\",\"\",sentence) # ...으로 끝나는 문장 제거(문장 잘림 유형)\n",
    "            sentence = re.sub(r\"\\[.*?\\]\",\"\",sentence) # 대괄호 및 단어 제거\n",
    "            sentence = re.sub(r\"www\\.[a-zA-Z0-9-]+\\.com\",\"\",sentence) # www.com 형태의 문장 제거\n",
    "            sentence = re.sub(r\"\\b\\w+\\.com\\b\",\"\",sentence) # .com 형태의 문장 제거\n",
    "            sentence = re.sub(r\"\\([^)]*persen[^)]*\\)\",\"\",sentence) #(persen...text) 형태의 문장 제거\n",
    "            sentence = re.sub(r\"\\([^)]*Entertainment[^)]*\\)\",\"\",sentence) #(Entertainment...text) 형태의 문장 제거\n",
    "            sentence = re.sub(r\".+\\s*\\.\\.\\.$\",\"\",sentence) #마지막이 ...로 끝나는 문장 모두 제거\n",
    "            sentence = re.sub(r\".+\\s*….$\",\"\",sentence)\n",
    "            sentence = re.sub(r\"\\(.*?\\)\",\"\",sentence) #()포함 소괄호 안에 있는 텍스트까지 제거\n",
    "\n",
    "            #하나씩 제거하는 정규표현식\n",
    "            \n",
    "            sentence = re.sub(r'gt;_lt;', '',sentence) #remove ;gt and ;lt\n",
    "            sentence = re.sub(r'gt;',\"\",sentence)\n",
    "            sentence = re.sub(r'lt;',\"\",sentence)\n",
    "            sentence = re.sub(r'_lt;',\"\",sentence)\n",
    "            sentence = re.sub(r\"Advertisements\",\"\",sentence) #remove english\n",
    "            sentence = re.sub(r\"Instagrammable\",\"\",sentence)\n",
    "            sentence = re.sub(r\"instagrammable\",\"\",sentence)\n",
    "            sentence = re.sub(r\"LOL\",\"\",sentence)\n",
    "            sentence = re.sub(r\"LoL\",\"\",sentence)\n",
    "            sentence = re.sub(r\"gantiin\",\"\",sentence) #remove wrong spelling\n",
    "            sentence = re.sub(r\"content]\",\"\",sentence)\n",
    "            sentence = re.sub(r\"@ star1\",\"\",sentence) #@star1 이라는 잡지회사 제거\n",
    "            sentence = re.sub(r\"@star1\",\"\",sentence)\n",
    "            sentence = re.sub(r\"♥\",\"\",sentence) #특수문자 제거\n",
    "            sentence = re.sub(r\"▲\",\"\",sentence)\n",
    "            sentence = re.sub(r\"=\",\"\",sentence)\n",
    "            sentence = re.sub(r\"#\",\"\",sentence) \n",
    "            sentence = re.sub(r\"@\",\"\",sentence) \n",
    "            sentence = re.sub(r\"\\[\",\"\",sentence) \n",
    "            sentence = re.sub(r\"]\",\"\",sentence) \n",
    "            sentence = re.sub(r\"~\",\"\",sentence) \n",
    "            sentence = re.sub(r\"\\^\",\"\",sentence) \n",
    "            sentence = re.sub(r\"→\",\"\",sentence) \n",
    "            sentence = re.sub(r\"-\",\"\",sentence)\n",
    "            spacy_doc = nlp(sentence)  # Renamed to spacy_doc for clarity\n",
    "            tokens = [token.text for sent in spacy_doc.sents for token in sent]\n",
    "            \n",
    "            # 문장, 토큰, 문장길이, Sen_ID 컬럼 추가\n",
    "            new_row = row.copy()\n",
    "            new_row['Sentence'] = sentence\n",
    "            new_row['Token'] = tokens\n",
    "            new_row['Sen_len'] = len(tokens)\n",
    "            new_row['Sen_ID'] = f\"{row['Doc_ID']}_sen{sen_id:06d}\"\n",
    "            \n",
    "            new_rows.append(new_row)\n",
    "            sen_id += 1  # 문장 ID 증가\n",
    "\n",
    "\n",
    "    # 리스트 기호 없이 단어 단위로 토큰화된 새로운 데이터 프레임 생성\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    # 토큰 길이 필터링\n",
    "    new_df = new_df[new_df['Token'].apply(lambda x: len(x) > max_length)]\n",
    "\n",
    "    new_df['Tokenized_Sentence']= new_df['Token'].apply(' '.join)\n",
    "    new_df['Pub_Type']='Newspaper'\n",
    "\n",
    "    # 'Sentence' 컬럼 중심으로 중복 제거\n",
    "    new_df.drop_duplicates(subset='Sentence', inplace=True)\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 공식 제출용 컬럼명으로 컬럼명 변환\n",
    "    new_df.rename(columns={'Source_File' : 'Filename', 'Headline':'Title', 'Keyword':'Pub_Subj', 'Date': 'Pub_Date', 'Sen_len': 'Word_Count'}, inplace=True)\n",
    "    new_df = new_df[['Doc_ID', 'Filename', 'Title', 'Pub_Type', 'Pub_Subj', 'Pub_Date', 'Col_Date', 'Sen_ID', 'Word_Count', 'Text', 'Sentence','Tokenized_Sentence' , 'Token']]\n",
    "    \n",
    "    # 특정 단어나 구문 제거\n",
    "    words_to_remove = ['/instagram', 'Written by', 'written by', 'WRITTEN BY', 'POWERED BY',\n",
    "                    'ads', 'full credits', 'Full Credits', 'FULL CREDITS']\n",
    "    new_df['Tokenized_Sentence'] = new_df['Tokenized_Sentence'].apply(remove_sentences_with_words, words=words_to_remove)\n",
    "    new_df['Tokenized_Sentence'].replace('', np.nan, inplace=True)\n",
    "    new_df.dropna(inplace=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "\n",
    "def remove_sentences_with_words(text, words):\n",
    "    if not text:\n",
    "        return \"\"  # 만약 텍스트가 비어있으면 빈 문자열을 반환\n",
    "\n",
    "    # 간단한 정규 표현식을 사용하여 텍스트를 문장으로 분리\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "    # 필터링된 문장을 저장하기 위한 리스트 초기화\n",
    "    filtered_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # 주어진 단어들 중 하나라도 문장에 존재하는지 확인 (대소문자 구분 없음)\n",
    "        if any(re.search(fr'\\b{re.escape(word)}\\b', sentence, re.IGNORECASE) for word in words):\n",
    "            continue  # 만약 특정 단어를 포함하는 문장이면 건너뜀\n",
    "        else:\n",
    "            filtered_sentences.append(sentence)  # 그렇지 않은 경우, 문장을 리스트에 추가\n",
    "\n",
    "    # 필터링된 문장들을 하나의 텍스트로 결합\n",
    "    result = ' '.join(filtered_sentences)\n",
    "    \n",
    "    return result  # 결과 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|          | 0/269 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 269/269 [04:08<00:00,  1.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Title</th>\n",
       "      <th>Pub_Type</th>\n",
       "      <th>Pub_Subj</th>\n",
       "      <th>Pub_Date</th>\n",
       "      <th>Col_Date</th>\n",
       "      <th>Sen_ID</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tokenized_Sentence</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230920_newsdata_Koreana2_000001</td>\n",
       "      <td>C:/Users/USER/Desktop/news data/20230920_data/...</td>\n",
       "      <td>Mempertahankan Rasa “Kelambanan”</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>Koreana2</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>20230920_newsdata_Koreana2_000001_sen000001</td>\n",
       "      <td>12</td>\n",
       "      <td>Fermentasi yang memerlukan waktu dan perawatan...</td>\n",
       "      <td>Fermentasi yang memerlukan waktu dan perawatan...</td>\n",
       "      <td>Fermentasi yang memerlukan waktu dan perawatan...</td>\n",
       "      <td>[Fermentasi, yang, memerlukan, waktu, dan, per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230920_newsdata_Koreana2_000001</td>\n",
       "      <td>C:/Users/USER/Desktop/news data/20230920_data/...</td>\n",
       "      <td>Mempertahankan Rasa “Kelambanan”</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>Koreana2</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>20230920_newsdata_Koreana2_000001_sen000003</td>\n",
       "      <td>32</td>\n",
       "      <td>Fermentasi yang memerlukan waktu dan perawatan...</td>\n",
       "      <td>Di antara sejumlah makanan fermentasi Korea ya...</td>\n",
       "      <td>Di antara sejumlah makanan fermentasi Korea ya...</td>\n",
       "      <td>[Di, antara, sejumlah, makanan, fermentasi, Ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20230920_newsdata_Koreana2_000001</td>\n",
       "      <td>C:/Users/USER/Desktop/news data/20230920_data/...</td>\n",
       "      <td>Mempertahankan Rasa “Kelambanan”</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>Koreana2</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>20230920_newsdata_Koreana2_000001_sen000004</td>\n",
       "      <td>25</td>\n",
       "      <td>Fermentasi yang memerlukan waktu dan perawatan...</td>\n",
       "      <td>Satu keluarga secara khusus telah mempertahank...</td>\n",
       "      <td>Satu keluarga secara khusus telah mempertahank...</td>\n",
       "      <td>[Satu, keluarga, secara, khusus, telah, memper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20230920_newsdata_Koreana2_000001</td>\n",
       "      <td>C:/Users/USER/Desktop/news data/20230920_data/...</td>\n",
       "      <td>Mempertahankan Rasa “Kelambanan”</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>Koreana2</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>20230920_newsdata_Koreana2_000001_sen000005</td>\n",
       "      <td>16</td>\n",
       "      <td>Fermentasi yang memerlukan waktu dan perawatan...</td>\n",
       "      <td>“Kami membuat meju dengan merebus kacang pada ...</td>\n",
       "      <td>“ Kami membuat meju dengan merebus kacang pada...</td>\n",
       "      <td>[“, Kami, membuat, meju, dengan, merebus, kaca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20230920_newsdata_Koreana2_000001</td>\n",
       "      <td>C:/Users/USER/Desktop/news data/20230920_data/...</td>\n",
       "      <td>Mempertahankan Rasa “Kelambanan”</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>Koreana2</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>20230920_newsdata_Koreana2_000001_sen000006</td>\n",
       "      <td>19</td>\n",
       "      <td>Fermentasi yang memerlukan waktu dan perawatan...</td>\n",
       "      <td>Setelah difermentasi selama sebulan, kami baru...</td>\n",
       "      <td>Setelah difermentasi selama sebulan , kami bar...</td>\n",
       "      <td>[Setelah, difermentasi, selama, sebulan, ,, ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21972</th>\n",
       "      <td>20230920_newsdata_Koreana2_000270</td>\n",
       "      <td>C:/Users/USER/Desktop/news data/20230920_data/...</td>\n",
       "      <td>Trashbusters: Gerakan Lingkungan sebagai Budaya</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>Koreana2</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>20230920_newsdata_Koreana2_000270_sen000072</td>\n",
       "      <td>18</td>\n",
       "      <td>Trashbusters tidak putus asa saat memandang tu...</td>\n",
       "      <td>Era yang memungkinkan apa saja dapat diantar m...</td>\n",
       "      <td>Era yang memungkinkan apa saja dapat diantar m...</td>\n",
       "      <td>[Era, yang, memungkinkan, apa, saja, dapat, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21973</th>\n",
       "      <td>20230920_newsdata_Koreana2_000270</td>\n",
       "      <td>C:/Users/USER/Desktop/news data/20230920_data/...</td>\n",
       "      <td>Trashbusters: Gerakan Lingkungan sebagai Budaya</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>Koreana2</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>20230920_newsdata_Koreana2_000270_sen000073</td>\n",
       "      <td>23</td>\n",
       "      <td>Trashbusters tidak putus asa saat memandang tu...</td>\n",
       "      <td>Meski penggunaan kantong plastik sudah dilaran...</td>\n",
       "      <td>Meski penggunaan kantong plastik sudah dilaran...</td>\n",
       "      <td>[Meski, penggunaan, kantong, plastik, sudah, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21974</th>\n",
       "      <td>20230920_newsdata_Koreana2_000270</td>\n",
       "      <td>C:/Users/USER/Desktop/news data/20230920_data/...</td>\n",
       "      <td>Trashbusters: Gerakan Lingkungan sebagai Budaya</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>Koreana2</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>20230920_newsdata_Koreana2_000270_sen000074</td>\n",
       "      <td>8</td>\n",
       "      <td>Trashbusters tidak putus asa saat memandang tu...</td>\n",
       "      <td>Halhal itu mungkin dapat memicu tawa sinis.</td>\n",
       "      <td>Halhal itu mungkin dapat memicu tawa sinis .</td>\n",
       "      <td>[Halhal, itu, mungkin, dapat, memicu, tawa, si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21975</th>\n",
       "      <td>20230920_newsdata_Koreana2_000270</td>\n",
       "      <td>C:/Users/USER/Desktop/news data/20230920_data/...</td>\n",
       "      <td>Trashbusters: Gerakan Lingkungan sebagai Budaya</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>Koreana2</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>20230920_newsdata_Koreana2_000270_sen000075</td>\n",
       "      <td>9</td>\n",
       "      <td>Trashbusters tidak putus asa saat memandang tu...</td>\n",
       "      <td>Meskipun demikian, mari kita mengucapkan sloga...</td>\n",
       "      <td>Meskipun demikian , mari kita mengucapkan slog...</td>\n",
       "      <td>[Meskipun, demikian, ,, mari, kita, mengucapka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21976</th>\n",
       "      <td>20230920_newsdata_Koreana2_000270</td>\n",
       "      <td>C:/Users/USER/Desktop/news data/20230920_data/...</td>\n",
       "      <td>Trashbusters: Gerakan Lingkungan sebagai Budaya</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>Koreana2</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>20230920_newsdata_Koreana2_000270_sen000076</td>\n",
       "      <td>33</td>\n",
       "      <td>Trashbusters tidak putus asa saat memandang tu...</td>\n",
       "      <td>Kemudian, mari kita mengingatkan diri kita bah...</td>\n",
       "      <td>Kemudian , mari kita mengingatkan diri kita ba...</td>\n",
       "      <td>[Kemudian, ,, mari, kita, mengingatkan, diri, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21977 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Doc_ID  \\\n",
       "0      20230920_newsdata_Koreana2_000001   \n",
       "1      20230920_newsdata_Koreana2_000001   \n",
       "2      20230920_newsdata_Koreana2_000001   \n",
       "3      20230920_newsdata_Koreana2_000001   \n",
       "4      20230920_newsdata_Koreana2_000001   \n",
       "...                                  ...   \n",
       "21972  20230920_newsdata_Koreana2_000270   \n",
       "21973  20230920_newsdata_Koreana2_000270   \n",
       "21974  20230920_newsdata_Koreana2_000270   \n",
       "21975  20230920_newsdata_Koreana2_000270   \n",
       "21976  20230920_newsdata_Koreana2_000270   \n",
       "\n",
       "                                                Filename  \\\n",
       "0      C:/Users/USER/Desktop/news data/20230920_data/...   \n",
       "1      C:/Users/USER/Desktop/news data/20230920_data/...   \n",
       "2      C:/Users/USER/Desktop/news data/20230920_data/...   \n",
       "3      C:/Users/USER/Desktop/news data/20230920_data/...   \n",
       "4      C:/Users/USER/Desktop/news data/20230920_data/...   \n",
       "...                                                  ...   \n",
       "21972  C:/Users/USER/Desktop/news data/20230920_data/...   \n",
       "21973  C:/Users/USER/Desktop/news data/20230920_data/...   \n",
       "21974  C:/Users/USER/Desktop/news data/20230920_data/...   \n",
       "21975  C:/Users/USER/Desktop/news data/20230920_data/...   \n",
       "21976  C:/Users/USER/Desktop/news data/20230920_data/...   \n",
       "\n",
       "                                                 Title   Pub_Type  Pub_Subj  \\\n",
       "0                     Mempertahankan Rasa “Kelambanan”  Newspaper  Koreana2   \n",
       "1                     Mempertahankan Rasa “Kelambanan”  Newspaper  Koreana2   \n",
       "2                     Mempertahankan Rasa “Kelambanan”  Newspaper  Koreana2   \n",
       "3                     Mempertahankan Rasa “Kelambanan”  Newspaper  Koreana2   \n",
       "4                     Mempertahankan Rasa “Kelambanan”  Newspaper  Koreana2   \n",
       "...                                                ...        ...       ...   \n",
       "21972  Trashbusters: Gerakan Lingkungan sebagai Budaya  Newspaper  Koreana2   \n",
       "21973  Trashbusters: Gerakan Lingkungan sebagai Budaya  Newspaper  Koreana2   \n",
       "21974  Trashbusters: Gerakan Lingkungan sebagai Budaya  Newspaper  Koreana2   \n",
       "21975  Trashbusters: Gerakan Lingkungan sebagai Budaya  Newspaper  Koreana2   \n",
       "21976  Trashbusters: Gerakan Lingkungan sebagai Budaya  Newspaper  Koreana2   \n",
       "\n",
       "         Pub_Date    Col_Date                                       Sen_ID  \\\n",
       "0      2020-01-01  2023-09-20  20230920_newsdata_Koreana2_000001_sen000001   \n",
       "1      2020-01-01  2023-09-20  20230920_newsdata_Koreana2_000001_sen000003   \n",
       "2      2020-01-01  2023-09-20  20230920_newsdata_Koreana2_000001_sen000004   \n",
       "3      2020-01-01  2023-09-20  20230920_newsdata_Koreana2_000001_sen000005   \n",
       "4      2020-01-01  2023-09-20  20230920_newsdata_Koreana2_000001_sen000006   \n",
       "...           ...         ...                                          ...   \n",
       "21972  2023-04-01  2023-09-20  20230920_newsdata_Koreana2_000270_sen000072   \n",
       "21973  2023-04-01  2023-09-20  20230920_newsdata_Koreana2_000270_sen000073   \n",
       "21974  2023-04-01  2023-09-20  20230920_newsdata_Koreana2_000270_sen000074   \n",
       "21975  2023-04-01  2023-09-20  20230920_newsdata_Koreana2_000270_sen000075   \n",
       "21976  2023-04-01  2023-09-20  20230920_newsdata_Koreana2_000270_sen000076   \n",
       "\n",
       "       Word_Count                                               Text  \\\n",
       "0              12  Fermentasi yang memerlukan waktu dan perawatan...   \n",
       "1              32  Fermentasi yang memerlukan waktu dan perawatan...   \n",
       "2              25  Fermentasi yang memerlukan waktu dan perawatan...   \n",
       "3              16  Fermentasi yang memerlukan waktu dan perawatan...   \n",
       "4              19  Fermentasi yang memerlukan waktu dan perawatan...   \n",
       "...           ...                                                ...   \n",
       "21972          18  Trashbusters tidak putus asa saat memandang tu...   \n",
       "21973          23  Trashbusters tidak putus asa saat memandang tu...   \n",
       "21974           8  Trashbusters tidak putus asa saat memandang tu...   \n",
       "21975           9  Trashbusters tidak putus asa saat memandang tu...   \n",
       "21976          33  Trashbusters tidak putus asa saat memandang tu...   \n",
       "\n",
       "                                                Sentence  \\\n",
       "0      Fermentasi yang memerlukan waktu dan perawatan...   \n",
       "1      Di antara sejumlah makanan fermentasi Korea ya...   \n",
       "2      Satu keluarga secara khusus telah mempertahank...   \n",
       "3      “Kami membuat meju dengan merebus kacang pada ...   \n",
       "4      Setelah difermentasi selama sebulan, kami baru...   \n",
       "...                                                  ...   \n",
       "21972  Era yang memungkinkan apa saja dapat diantar m...   \n",
       "21973  Meski penggunaan kantong plastik sudah dilaran...   \n",
       "21974        Halhal itu mungkin dapat memicu tawa sinis.   \n",
       "21975  Meskipun demikian, mari kita mengucapkan sloga...   \n",
       "21976  Kemudian, mari kita mengingatkan diri kita bah...   \n",
       "\n",
       "                                      Tokenized_Sentence  \\\n",
       "0      Fermentasi yang memerlukan waktu dan perawatan...   \n",
       "1      Di antara sejumlah makanan fermentasi Korea ya...   \n",
       "2      Satu keluarga secara khusus telah mempertahank...   \n",
       "3      “ Kami membuat meju dengan merebus kacang pada...   \n",
       "4      Setelah difermentasi selama sebulan , kami bar...   \n",
       "...                                                  ...   \n",
       "21972  Era yang memungkinkan apa saja dapat diantar m...   \n",
       "21973  Meski penggunaan kantong plastik sudah dilaran...   \n",
       "21974       Halhal itu mungkin dapat memicu tawa sinis .   \n",
       "21975  Meskipun demikian , mari kita mengucapkan slog...   \n",
       "21976  Kemudian , mari kita mengingatkan diri kita ba...   \n",
       "\n",
       "                                                   Token  \n",
       "0      [Fermentasi, yang, memerlukan, waktu, dan, per...  \n",
       "1      [Di, antara, sejumlah, makanan, fermentasi, Ko...  \n",
       "2      [Satu, keluarga, secara, khusus, telah, memper...  \n",
       "3      [“, Kami, membuat, meju, dengan, merebus, kaca...  \n",
       "4      [Setelah, difermentasi, selama, sebulan, ,, ka...  \n",
       "...                                                  ...  \n",
       "21972  [Era, yang, memungkinkan, apa, saja, dapat, di...  \n",
       "21973  [Meski, penggunaan, kantong, plastik, sudah, d...  \n",
       "21974  [Halhal, itu, mungkin, dapat, memicu, tawa, si...  \n",
       "21975  [Meskipun, demikian, ,, mari, kita, mengucapka...  \n",
       "21976  [Kemudian, ,, mari, kita, mengingatkan, diri, ...  \n",
       "\n",
       "[21977 rows x 13 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = tokenizing(df_all)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 테스트웍스(가공 담당 회사) 전달 전 각 Token에 대한 column 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰별 작업용 컬럼 생성\n",
    "for i in range(df['Word_Count'].max()):\n",
    "    new_column_name = f'Column_{i}'  # 새 열 이름 생성\n",
    "    df[new_column_name]=np.nan\n",
    "\n",
    "for idx in tqdm(df.index):\n",
    "    for i, token in enumerate(df['Token'][idx]):\n",
    "        new_column_name = f'Column_{i}'  # 새 열 이름 생성\n",
    "        df[new_column_name][idx]=token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Sen_ID'기준으로 정렬\n",
    "df.sort_values('Sen_ID', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. pickle 형태로 정제 완료 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜_newsdata_원천데이터 합본.pickle\n",
    "df.to_pickle('20231111_news_assemble_unique.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
